{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import segmentation_models as sm\n",
    "import gc\n",
    "from pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Put this in pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def predict_and_assemble(self, volume, mask, threshold, model):\n",
    "#     test_a_ds = self.make_iterated_data_generator(volume, mask)\n",
    "#     locations = self.list_all_locations(mask)\n",
    "#     all_pred = np.zeros(mask.shape)\n",
    "#     all_binary_pred = np.zeros(mask.shape)\n",
    "#     pred = []\n",
    "\n",
    "#     for i, patch in tqdm(enumerate(test_a_ds())):\n",
    "#         # pred[i]: (128, 128, 1)\n",
    "#         p = model.predict(np.expand_dims(patch, 0))[0]\n",
    "#         pred.append(p)\n",
    "#         c = locations[i]\n",
    "#         l = c[0] - self.patch_halfsize\n",
    "#         r = c[0] + self.patch_halfsize\n",
    "#         t = c[1] + self.patch_halfsize\n",
    "#         b = c[1] - self.patch_halfsize\n",
    "#         p = p.squeeze()\n",
    "#         all_pred[l:r, b:t] = p\n",
    "#         p = p > threshold\n",
    "#         all_binary_pred[l:r, b:t] = p\n",
    "\n",
    "#     return all_pred, all_binary_pred, pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test set, build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./dataset/\"\n",
    "patch_size = 128  # e.g. 128x128\n",
    "downsampling = 0.75  # setting this to e.g. 0.5 means images will be loaded as 2x smaller. 1 does nothing.\n",
    "z_dim = 40   # number of slices in the z direction. max value is 65 - z_start\n",
    "z_start = 0  # offset of slices in the z direction\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "steps_per_epoch = 50\n",
    "val_step = 50\n",
    "\n",
    "pipeline = Pipeline(data_dir, patch_size, downsampling, z_dim, z_start, batch_size)\n",
    "volume_a, mask_a, _ = pipeline.load_sample(split=\"test\", index='a')\n",
    "# volume_b, mask_b, _ = pipeline.load_sample(split=\"test\", index='b')\n",
    "\n",
    "gc.collect()\n",
    "print(\"Loading complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"volume_a: {volume_a.shape}\")\n",
    "print(f\"mask_a: {mask_a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet(\n",
    "    'resnet50',\n",
    "    input_shape=pipeline.get_input_shape(),\n",
    "    encoder_weights=None,\n",
    "    classes=1\n",
    ")\n",
    "model.load_weights('chkpt/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "all_pred, all_binary_pred, pred = pipeline.predict_and_assemble(volume_a, mask_a, threshold, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask\n",
    "plt.imshow(mask_a, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary predictions\n",
    "plt.imshow(all_binary_pred, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-binary predictions\n",
    "plt.imshow(all_pred, cmap='gray')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rle(output):\n",
    "#     flat_img = np.where(output > 0.4, 1, 0).astype(np.uint8)\n",
    "#     starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n",
    "#     ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n",
    "#     starts_ix = np.where(starts)[0] + 2\n",
    "#     ends_ix = np.where(ends)[0] + 2\n",
    "#     lengths = ends_ix - starts_ix\n",
    "#     return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission = defaultdict(list)\n",
    "# for fragment_id, fragment_name in enumerate(test_fragments):\n",
    "#     submission[\"Id\"].append(fragment_name.name)\n",
    "#     submission[\"Predicted\"].append(rle(pred_images[fragment_id]))\n",
    "\n",
    "# pd.DataFrame.from_dict(submission).to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
